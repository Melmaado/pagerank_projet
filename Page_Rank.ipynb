{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97235a4",
   "metadata": {},
   "source": [
    "# PageRank\n",
    "\n",
    "On calcule PageRank avec la méthode de puissance :\n",
    "\n",
    "$$p^{(k+1)} = T \\cdot p^{(k)}$$\n",
    "\n",
    "Principe :\n",
    "\n",
    "- T[i, j] = probabilité d'aller de la page j vers la page i (colonne = page de départ)\n",
    "- Si une page n'a aucun lien sortant (impasse) : prochaine page uniforme (1/N)\n",
    "- Arrêt quand :\n",
    "\n",
    "$$\\frac{||T p^{(k)} - p^{(k)}||_1}{||p^{(k)}||_1} \\leq \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e57ff628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7a21d",
   "metadata": {},
   "source": [
    "## 1) Lire les CSV\n",
    "- names.csv : colonne Name (noms de pages)\n",
    "- edges.csv : colonnes FromNode, ToNode (liens entre pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f8b9824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199903, 10722190)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste_pages = pd.read_csv(\"names.csv\")     # Name\n",
    "liste_liens = pd.read_csv(\"edges.csv\")       # FromNode, ToNode\n",
    "\n",
    "nombre_pages = len(liste_pages)\n",
    "\n",
    "pages_depart = liste_liens[\"FromNode\"].to_numpy() - 1   # j\n",
    "pages_arrivee = liste_liens[\"ToNode\"].to_numpy() - 1     # i\n",
    "\n",
    "nombre_pages, len(liste_liens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a56190a",
   "metadata": {},
   "source": [
    "## 2) Degrés sortants + sinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a7bf188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de pages sans liens sortants (impasses) : 0\n"
     ]
    }
   ],
   "source": [
    "nbr_liens_page = np.bincount(pages_depart, minlength=nombre_pages)\n",
    "impasses = (nbr_liens_page == 0)\n",
    "\n",
    "print(f\"Nombre de pages sans liens sortants (impasses) : {impasses.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf811b",
   "metadata": {},
   "source": [
    "## 3) Construction de la matrice de transition $T$\n",
    "\n",
    "Le dataset étant lourd, on utilise une **matrice creuse** de SciPy pour optimiser.\n",
    "\n",
    "* **Format utilisé** : `csr_matrix` (Compressed Sparse Row), idéal pour les multiplications matrice-vecteur.\n",
    "* **Contenu de $T$** : Si un lien existe de la page $j$ vers la page $i$, l'élément $T_{ij}$ reçoit la probabilité $$\\frac{1}{nbr\\_liens\\_page[j]}$$\n",
    "* **Gestion des impasses** : Les pages sans liens sortants ne sont pas stockées dans la matrice creuse pour économiser de l'espace. Leur poids est redistribué dynamiquement lors de l'itération pour simuler un saut aléatoire et de probabilité égale vers n'importe quelle page du réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c77131a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction de la partie \"liens\" de la matrice de transition\n",
    "# Chaque lien j -> i reçoit une probabilité de 1 / nbr_liens_page[j]\n",
    "proba_liens_page = 1.0 / nbr_liens_page[pages_depart]\n",
    "T = csr_matrix((proba_liens_page, (pages_arrivee, pages_depart)), shape=(nombre_pages, nombre_pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211a4f8b",
   "metadata": {},
   "source": [
    "## 4) Méthode de puissance avec facteur d'amortissement ($d$)\n",
    "\n",
    "L'objectif est de calculer le vecteur de PageRank $\\vec{p}^{(\\infty)}$ par itérations successives. Pour éviter les boucles, on utilise un **facteur d'amortissement $d$** (fixé ici à $0,85$).\n",
    "\n",
    "La formule d'itération combinant la matrice de transition $T$ et le saut aléatoire est :\n",
    "\n",
    "$$\\vec{p}^{(k+1)} = d \\cdot (T \\cdot \\vec{p}^{(k)}) + \\frac{1 - d}{N} \\cdot \\vec{1}$$\n",
    "\n",
    "* **$d$** : représente la probabilité qu'un utilisateur continue sa navigation en suivant les liens\n",
    "* **$T \\cdot \\vec{p}^{(k)}$** : gère la multiplication par la matrice creuse et la redistribution du poids des \"impasses\".\n",
    "* **$(1 - d) / N$** : représente la probabilité pour un utilisateur de sauter vers n'importe quelle page du réseau de manière uniforme.\n",
    "\n",
    "### Critère d'arrêt\n",
    "L'algorithme s'arrête lorsque la variation relative entre deux itérations devient inférieure ou égale à un seuil $\\epsilon$ :\n",
    "\n",
    "$$\\frac{\\|T\\vec{p}^{(k)} - \\vec{p}^{(k)}\\|_1}{\\|\\vec{p}^{(k)}\\|_1} \\leq \\epsilon$$\n",
    "\n",
    "Où la norme 1 est définie par $\\|v\\|_1 = \\sum_{i=1}^{n} |v_i|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd943818",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-9\n",
    "k = 0\n",
    "p_k = np.ones(nombre_pages) / nombre_pages\n",
    "d = 0.85\n",
    "\n",
    "while True:\n",
    "    # p_kplus1 = (Partie creuse des liens) * p_k + (Partie uniforme des impasses)\n",
    "    # L'opérateur @ déclenche la multiplication optimisée de SciPy\n",
    "    p_kplus1 = d*(T @ p_k + p_k[impasses].sum()/nombre_pages) + (1 - d) / nombre_pages\n",
    "\n",
    "    err = np.abs(p_kplus1 - p_k).sum() / np.abs(p_k).sum()\n",
    "    p_k = p_kplus1\n",
    "    k += 1\n",
    "\n",
    "    if err <= eps:\n",
    "        break\n",
    "\n",
    "print(f\"Convergence atteinte en {k} itérations avec une erreur de {err:.2e}.\")\n",
    "print(f\"Somme de vérification du vecteur p : {p_k.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d65d5c",
   "metadata": {},
   "source": [
    "## 5) Top pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "idx = np.argsort(-p_k)[:top_k] # on trie par PageRank et on prend les k premiers par ordre DECROISSANT\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"rank\": np.arange(1, top_k + 1),\n",
    "    \"node_id\": idx + 1,\n",
    "    \"name\": liste_pages[\"Name\"].iloc[idx].to_numpy(),\n",
    "    \"pagerank\": p_k[idx],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea45bf5",
   "metadata": {},
   "source": [
    "## 6) Recherche basique (titre contient le mot-clé)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d11132",
   "metadata": {},
   "source": [
    "On filtre par nom et trie par score PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55611ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recherche = \"New York\"\n",
    "top_k = 10\n",
    "\n",
    "recherche = recherche.lower()\n",
    "mask = liste_pages[\"Name\"].str.lower().str.contains(recherche, na=False).to_numpy() # on filtre les pages contenant le mot clé\n",
    "idx = np.where(mask)[0] # on récupère les indices des pages correspondantes\n",
    "idx = idx[np.argsort(-p_k[idx])][:top_k] # on trie par PageRank et on prend les k premiers par ordre DECROISSANT\n",
    "\n",
    "resultat = pd.DataFrame({\n",
    "    \"rank\": np.arange(1, len(idx) + 1),\n",
    "    \"node_id\": idx + 1,\n",
    "    \"name\": liste_pages[\"Name\"].iloc[idx].to_numpy(),\n",
    "    \"pagerank\": p_k[idx],\n",
    "})\n",
    "resultat\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
